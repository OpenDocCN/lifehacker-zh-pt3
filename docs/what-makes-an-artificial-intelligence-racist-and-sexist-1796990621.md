# 是什么让人工智能成为种族主义者和性别歧视者

> 原文：<https://lifehacker.com/what-makes-an-artificial-intelligence-racist-and-sexist-1796990621>

人工智能正在渗透我们的日常生活，应用程序 [管理你的手机照片](https://lifehacker.com/this-app-can-choose-your-best-photos-for-you-1796369992)[管理你的电子邮件](https://lifehacker.com/this-app-uses-artificial-intelligence-to-manage-your-in-1796211455)[将文本从任何语言翻译成另一种语言](https://lifehacker.com/google-translate-will-now-use-neural-learning-to-make-e-1789036349) 。谷歌、脸书、苹果和微软都在大力研究如何将人工智能集成到他们的主要服务中。很快，你可能会在每次拿起手机时与人工智能(或其输出)互动。你应该相信它吗？不总是。



人工智能可以比人类更快更准确地分析数据，但它也可以继承我们的偏见。为了学习，它需要大量的数据，而找到这些数据最简单的方法就是从互联网上输入文本。但是网络上包含了一些极其偏颇的语言。斯坦福大学的一项研究 发现，一个经过互联网训练的人工智能将刻板的白人名字与“爱”等积极词汇联系起来，将黑人名字与“失败”和“癌症”等消极词汇联系起来

[Luminoso](https://luminoso.com/) 首席科学官 Rob Speer 负责监管开源数据集[concept net number batch](https://blog.conceptnet.io/2016/05/25/conceptnet-numberbatch-a-new-name-for-the-best-word-embeddings-you-can-download/)，该数据集被用作 AI 系统的知识库。他测试了 Numberbatch 的一个数据源，发现 [的单词关联](https://blog.conceptnet.io/2017/04/24/conceptnet-numberbatch-17-04-better-less-stereotyped-word-vectors/) 存在明显问题。当被问到“男人对于女人就像店主对于…”系统填的是“家庭主妇”它同样将女性与缝纫和化妆品联系在一起。

虽然这些关联可能适用于某些应用程序，但它们会在评估求职者等常见的人工智能任务中引起问题。人工智能不知道哪些联系是有问题的，所以它可以毫无问题地将一份女性简历排在一份男性简历之后。类似地，当 Speer 试图建立一个餐馆评论算法时，它对墨西哥食物的评级较低，因为它已经学会了将“墨西哥”与“非法”等负面词汇联系起来。

于是施佩尔进去 [去偏概念网](https://blog.conceptnet.io/2017/04/24/conceptnet-numberbatch-17-04-better-less-stereotyped-word-vectors/) 。他识别出不恰当的联想，并将其调整为零，同时保持像“男人/叔叔”和“女人/阿姨”这样的恰当联想。他对与种族、民族和宗教相关的词汇也做了同样的处理。为了对抗人类的偏见，需要一个人类。

Speer 在一封电子邮件中说，Numberbatch 是唯一一个内置去偏置的语义数据库。他为这种竞争优势感到高兴，但他希望其他知识库也能效仿:

> 这是近期 AI 的威胁。这不是什么机器人接管世界的科幻场景。这是人工智能服务做出我们不理解的决定，这些决定最终会伤害某些人群。

这种偏见最可怕的地方在于它会在无形中接管一切。据斯佩尔说，“有些人(将)一生都不知道为什么他们得到的机会更少，工作机会更少，与警察或运输安全管理局的互动更多……”当然，他指出，种族主义和性别歧视是社会固有的，而有希望的技术进步，即使是明确要抵消它们，也常常会放大它们。没有建立在主观数据上的客观工具。因此，人工智能开发者肩负着巨大的责任，要找到他们的人工智能中的缺陷，并解决它们。

“应该更多地了解什么是真实的，什么是炒作，”施佩尔说。“夸大人工智能很容易，因为大多数人还没有正确的隐喻来理解它，这阻止了人们适当的怀疑。

“没有像人脑一样工作的人工智能，”他说。“为了反驳炒作，我希望我们可以停止谈论大脑，开始讨论实际发生的事情:它主要是统计学、数据库和模式识别。这不应该降低它的趣味性。”