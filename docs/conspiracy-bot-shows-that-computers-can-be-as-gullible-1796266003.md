# 阴谋机器人表明，计算机可以像人类一样容易受骗

> 原文：<https://lifehacker.com/conspiracy-bot-shows-that-computers-can-be-as-gullible-1796266003>

计算机现在相信阴谋论。 [*新调查*的 Francis Tseng 训练了一个机器人](https://thenewinquiry.com/you-probably-think-this-bot-is-about-you/) 来识别照片中的模式，并在相似的照片之间画出链接，形成那种在*国土*剧集的最后一幕或 Reddit 的首页看到的阴谋论图。这是一个提醒我们人类容易受骗的可爱把戏(嘿，也许那些照片*和*匹配！)，而且我们训练出来替我们思考的机器最终也会同样容易受骗。

Watch

人类 [格外擅长模式识别](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4141622/) 。这对于学习和处理不同的环境来说很棒，但它可能会给我们带来麻烦: [一些研究](https://www.psychologytoday.com/blog/naturally-selected/201412/why-our-brains-are-hardwired-conspiracy-theories) 将模式识别与阴谋论的信仰联系起来。( [有些不](https://arstechnica.com/science/2015/10/do-conspiracy-theorists-see-more-patterns-in-randomness-apparently-not/) ，但那是他们希望你认为的。)

直到最近，计算机还不太擅长模式匹配。机器学习的兴起专门针对弥合这一差距，通过向神经网络提供大量数据，教它们如何识别鸟类的照片 或。

这不像复制人脑那么简单，因为我们不知道如何去做。相反，程序员通过让神经网络自己搜索模式来模拟类似大脑的行为。 [正如技术专家大卫·温伯格写的](https://backchannel.com/our-machines-now-have-knowledge-well-never-understand-857a479dcc0e) ，这些神经网络摆脱了人类思想的包袱，建立了自己的逻辑，发现了令人惊讶和难以理解的模式。比如谷歌的 AlphaGo 可以打败一个围棋高手，但是它的策略并不能简单的用通俗易懂的语言解释。

但这些机器实际上并不知道什么是真实的，所以它们可以很容易地找到不存在或无关紧要的模式。这也导致了令人惊讶的“错误”，比如科学家贾内尔·谢恩(Janelle Shane)生成的有趣的油漆颜色(stummy beige，stanky bean)，或者谷歌 DeepDream 在我的自拍中发现的可怕的狗脸:

这些错误可能要严重得多。温伯格重点介绍了软件，以及中央情报局系统 [错误地认定一名半岛电视台记者为恐怖分子](https://arstechnica.co.uk/security/2016/02/the-nsas-skynet-program-may-be-killing-thousands-of-innocent-people/) 。

*新的调查*的机器人通过寻找虚假模式同样过度扩展了它的分析。“如果两张脸或两个物体看起来足够相似，机器人就会把它们联系起来，”机器人的创造者曾说。“这些感知失误不是作为错误出现，而是作为重大发现出现，鼓励人类从随机性中解读多层意义。”

人们很容易认为机器人有所发现。但很有可能，你真的只是在看狗脸和化妆的油漆颜色。计算机程序越像人类，在了解它们是如何被制造和训练之前，你就越不应该相信它们。见鬼，永远不要相信一台行为像人类的计算机。这就是你的阴谋论。